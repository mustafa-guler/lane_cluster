<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Slurm Basics on Lane Cluster Documentation</title><link>https://mustafa-guler.github.io/lane_cluster/slurm-basics/</link><description>Recent content in Slurm Basics on Lane Cluster Documentation</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://mustafa-guler.github.io/lane_cluster/slurm-basics/index.xml" rel="self" type="application/rss+xml"/><item><title>Connecting to Lane</title><link>https://mustafa-guler.github.io/lane_cluster/slurm-basics/connecting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_cluster/slurm-basics/connecting/</guid><description>The Lane cluster has the host name lanec1.compbio.cs.cmu.edu. You can connect to Lane via ssh with the command
ssh USERNAME@lanec1.compbio.cs.cmu.edu for Linux and OSX systems. If you are using Windows you can either use PuTTY or WSL. All following instructions operate under the assumption you use WSL if you&amp;rsquo;re a Windows user.
For a more compact representation you can create directory ~/.ssh if it doesn&amp;rsquo;t exist and store login information for Lane as follows.</description></item><item><title>Interactive Sessions</title><link>https://mustafa-guler.github.io/lane_cluster/slurm-basics/interactive-sessions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_cluster/slurm-basics/interactive-sessions/</guid><description>The way clusters are generally designed is there is a single, relatively small computer used as a &amp;ldquo;head&amp;rdquo; node. Users when executing ssh are actually connecting to this head node. Since this node handles all the initial user connection traffic nothing computationally intensive should be run by users on the head node. Processes that use large amounts of memory, CPU, or execute a lot of IO commands on the head node slow logins for other users severely.</description></item><item><title>Examining Partitions</title><link>https://mustafa-guler.github.io/lane_cluster/slurm-basics/examining-partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_cluster/slurm-basics/examining-partitions/</guid><description>When logging into a new Slurm cluster you can examine characteristics of available partitions with the sinfo command. Below are some useful examples for Lane. Your available partition lists may look different.
# List names of partitions you have access to sinfo -o &amp;#34;%20P&amp;#34; # output: # # PARTITION # DEBUG # short1 # interactive # moh1 # moh2-gpu # pool1 # pool3-bigmem # List total resources for each partition # CPUS and MEMORY are given per node # NODES are given in the format Allocated/Idle sinfo -o &amp;#34;%20P %10A %10c %10m %25f %20G %20N&amp;#34; # output: # # PARTITION NODES(A/I) CPUS MEMORY AVAIL_FEATURES GRES NODELIST # DEBUG 0/0 0 0 (null) (null) # short1 0/3 8 32000+ rack-0,8CPUs (null) compute-0-[9-11] # short1 1/2 32 64153 rack-3,32CPUs (null) compute-3-[32-34] # interactive 0/3 8 32000+ rack-0,8CPUs (null) compute-0-[9-11] # moh1 2/3 48 257364 rack-0,48CPUs (null) compute-0-[7,20-21,2 # moh1 0/2 56 128530 rack-1,56CPUs (null) compute-1-[2-6] # moh2-gpu 0/1 32 257348 rack-4,32CPUs, gpu:4 compute-4-18 # pool1 0/4 16 48059 rack-0,16CPUs (null) compute-0-[4-6,8] # pool1 0/8 8 23936 rack-0,8CPUs (null) compute-0-[22-28,30] # pool1 0/11 8 23933+ rack-1,8CPUs (null) compute-1-[7-10,20-2 # pool1 0/5 16 48059 rack-1,16CPUs (null) compute-1-[14-18] # pool3-bigmem 0/1 32 257757 rack-0,32CPUs (null) compute-0-12 # pool3-bigmem 0/1 8 128761 rack-1,8CPUs (null) compute-1-35 # List free resources # CPUS in the format Allocated/Idle/Other/Total sinfo -o &amp;#34;%20P %20e %20m %20C %f&amp;#34; # output: # # PARTITION FREE_MEM MEMORY CPUS(A/I/O/T) AVAIL_FEATURES # DEBUG 0 0 0/0/0/0 (null) # short1 29863-45543 32000+ 0/24/0/24 rack-0,8CPUs # short1 39072-61998 64153 16/80/0/96 rack-3,32CPUs # interactive 29863-45543 32000+ 0/24/0/24 rack-0,8CPUs # moh1 29816-250476 257364 148/92/240/480 rack-0,48CPUs # moh1 34069-94091 128530 0/168/112/280 rack-1,56CPUs # moh2-gpu 167177 257348 1/31/0/32 rack-4,32CPUs, # pool1 13063-N/A 23933+ 2/86/8/96 rack-1,8CPUs # pool1 40995-45646 48059 0/64/0/64 rack-0,16CPUs # pool1 21716-23147 23936 0/64/0/64 rack-0,8CPUs # pool1 30473-45375 48059 3/77/0/80 rack-1,16CPUs # pool3-bigmem 37394 128761 1/7/0/8 rack-1,8CPUs # pool3-bigmem 254574 257757 0/32/0/32 rack-0,32CPUs Shared Partitions on Lane Partition Name Description short1 I&amp;rsquo;ve never used this.</description></item><item><title>Storage on Lane</title><link>https://mustafa-guler.github.io/lane_cluster/slurm-basics/storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_cluster/slurm-basics/storage/</guid><description>There are a few storage systems on Lane. First is your home directory (~, /home/$(whoami)). Home directories have 500GB are by default are only accessible by you. You can examine characteristics of your home directory with the command
df -ah | grep -P &amp;#34;(Filesystem|$(whoami))&amp;#34; # output: # # Filesystem Size Used Avail Use% Mounted on # nas-3-31:/volume01/home/mguler 500G 54G 447G 11% /home/mguler In addition to your home directory you will likely have access to network attached storage (NAS) filesystems specific to your lab.</description></item><item><title>Stopping Jobs</title><link>https://mustafa-guler.github.io/lane_cluster/slurm-basics/stopping-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_cluster/slurm-basics/stopping-jobs/</guid><description>If you want to stop your job first find your job ID with squeue then run scancel JOBID. For array jobs using just the job ID before the underscore you will cancel all array tasks in the array job. Using the underscore you will only cancel a single array step.
External Resources scancel man page</description></item><item><title>Defining Terms</title><link>https://mustafa-guler.github.io/lane_cluster/slurm-basics/defining-terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_cluster/slurm-basics/defining-terms/</guid><description>There are some terms I use throughout that should be explained first to remove ambiguity.
Term Definition cluster A remote server that consists of a collection of computers resources CPUs, memory, GPUs, etc. that can be consumed by a program node A single computer in a cluster, contains resources partition A collection of nodes in a cluster with shared characteristics job Some executable submitted to the cluster to run on some partition with some resources.</description></item></channel></rss>