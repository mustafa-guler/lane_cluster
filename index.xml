<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Lane Cluster Documentation</title><link>https://mustafa-guler.github.io/lane_custer/</link><description>Recent content on Lane Cluster Documentation</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://mustafa-guler.github.io/lane_custer/index.xml" rel="self" type="application/rss+xml"/><item><title>A Simple sbatch Script</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/simple-script/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/simple-script/</guid><description>Consider the following shell script.
#!/bin/bash # contents of example_script.sh seq 1 10000 | shuf | sort -n This prints numbers from 1 to 10000 (seq), randomly shuffles them (shuf), then attempts to resort them. How could we submit this as a job? The simplest answer is to just execute sbatch with this script as a parameter.
sbatch -p pool1 -n 1 example_script.sh This will request one task (-n 1) on the pool1 partition (-p pool1) and run our script on it.</description></item><item><title>Connecting to Lane</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/connecting/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/connecting/</guid><description>The Lane cluster has the host name lanec1.compbio.cs.cmu.edu. You can connect to Lane via ssh with the command
ssh USERNAME@lanec1.compbio.cs.cmu.edu for Linux and OSX systems. If you are using Windows you can either use PuTTY or WSL. All following instructions operate under the assumption you use WSL if you&amp;rsquo;re a Windows user.
For a more compact representation you can create directory ~/.ssh if it doesn&amp;rsquo;t exist and store login information for Lane as follows.</description></item><item><title>Creating Conda Environments</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/creating-envs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/creating-envs/</guid><description>You can create conda environments with the conda create command. For more information about the last example see Transferring Conda Environments.
# create an environment named py27 that contains python 2.7 conda create -n py27 python=2.7 # create an environment named rdkit that contains a python 3 version and compatible rdkit installation conda create -n rdkit -c conda-forge python=3 rdkit # copy the rdkit environment to another environment named new_rdkit conda create -n new_rdkit --clone rdkit # create environment from environment.</description></item><item><title>Modulefile Directories</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/modulefile-dirs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/modulefile-dirs/</guid><description>The way available modules are determined is by modulefiles in directories. You can add your own modulefile directory with module use PATH/TO/DIR. Let&amp;rsquo;s use my own modulefile directory and see what we have.
module use /projects/mohimanilab/mustafa/tools/modulefiles module avail # output (only new section) # # ----------------------------------------------- /projects/mohimanilab/mustafa/tools/modulefiles ----------------------------------------------- # cudatoolkit/10.0 cudnn/6.0 fingerprinter tfcuda/1.10 tfcuda/1.15 tfcuda/1.6 tfcuda/2.1 # cudatoolkit/10.1 cudnn/7.4 proteowizard tfcuda/1.11 tfcuda/1.2 tfcuda/1.7 tfcuda/2.2 # cudatoolkit/8.0 cudnn/7.6 rust tfcuda/1.12 tfcuda/1.3 tfcuda/1.</description></item><item><title>rsync</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/transferring-data/rsync/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/transferring-data/rsync/</guid><description>This is another cp-like command with support for file transfer over networks. However, as the name suggests, this is a sync operation. This means that if some files exist unmodified in both the source and destination they will not be copied, saving some time. Additionally, rsync is considerably more flexible than scp with the trade-off of being a bit more complicated to use. Just like scp, rsync respects ssh configurations. The man page for rsync includes many examples that are useful.</description></item><item><title>Interactive Sessions</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/interactive-sessions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/interactive-sessions/</guid><description>The way clusters are generally designed is there is a single, relatively small computer used as a &amp;ldquo;head&amp;rdquo; node. Users when executing ssh are actually connecting to this head node. Since this node handles all the initial user connection traffic nothing computationally intensive should be run by users on the head node. Processes that use large amounts of memory, CPU, or execute a lot of IO commands on the head node slow logins for other users severely.</description></item><item><title>Modulefile Format</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/modulefile-fmt/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/modulefile-fmt/</guid><description>We showed previously modulefiles must begin with the magic cookie #%Module, but what do modulefiles consist of? Modulefiles are Tcl scripts that modify a user&amp;rsquo;s environment. Let&amp;rsquo;s look at my rust module as an example.
# contents of /projects/mohimanilab/mustafa/tools/modulefiles/rust #%Module ## ## Rust modulefile ## ## modulefiles/ ## set ver 1.39.0 set msg &amp;#34;This module adds necessary paths and variables to use rust&amp;#34; proc ModulesHelp { } { puts stderr $msg } module-whatis &amp;#34;Use rust $ver&amp;#34; prepend-path PATH /projects/mohimanilab/mustafa/cargo/bin This is a very simple modulefile.</description></item><item><title>Useful sbatch Flags</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/useful-flags/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/useful-flags/</guid><description>In the following table X is an integer and S is a string.
Flag Value Format Description -p S Partition that job should be submitted to. Can be a comma-separated list if you aren&amp;rsquo;t concerned with forcing a particular partition -n X Number of concurrent tasks you will run for this job. Tasks can be allocated from within your job script with srun -c X Number of CPUs each task will use for this job --mem-per-cpu X[KMGT] for KB,MB,GB,TB respectively Amount of memory per CPU this job requires --mem X[KMGT] for KB,MB,GB,TB respectively Amount of memory this job requires total, not per CPU -t X-XX:XX:XX Sets a maximum running time for your job.</description></item><item><title>Using Conda Environments</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/using-envs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/using-envs/</guid><description>After you have created an environment you need to activate it before you can access its contents. If you are running an interactive job you can do this via conda activate ENV_NAME. However, this does not always work as expected when it is part of a job submitting with sbatch (See Submitting Jobs for more information on job submission). You can work around this by activating a conda environment with the command</description></item><item><title>Array Jobs</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/array-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/array-jobs/</guid><description>What if I want to perform the same computation on a very large number of inputs? The Slurm solution to this is an array job. An array job executes the same script many times. It is specified with the sbatch flag -a. For each integer value of -a Slurm will create a distinct job and within that job set the environment variable SLURM_ARRAY_TASK_ID to the associated value of -a.
Example -a Explanation -a 1-100 Run an array job with array values 1,2,3,&amp;hellip;,100 -a 100-200 Run an array job with array values 100,102,103,&amp;hellip;,200 -a 0-15:4 Run an array job with array values 0,4,8,12 -a 0-15:2 Run an array job with array values 0,2,4,6,8,10,12,14 -a 0-15%2 Run an array job with array values 0,1,2,3,&amp;hellip;,15 but only ever have 2 running simultaneously There are limitations to array jobs.</description></item><item><title>Transferring Conda Environments</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/transferring-envs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/transferring-envs/</guid><description>Once you have set up an environment the way you like it you may want to use the same environment on a different machine. If the other machine is running the same overarching OS as yours (running Linux locally and remotely is the most common case of this) you can simply run
conda activate ENV_NAME conda env export | grep -v &amp;#34;^prefix&amp;#34; &amp;gt; environment.yaml This will create an environment.yaml file that contains all the packages you install in this conda environment along with their frozen versions.</description></item><item><title>Using Modules</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/using-modules/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/using-modules/</guid><description>Once you have added a modulefile directory with module use you can load modules with module load and unload them with module unload. At any point you can use module list to see what modules you have active. Additionally, you can use module purge to unload all loaded modules. You can add module load commands to the beginning of jobs scripts to use modules in your jobs. For more information on submitting jobs see Submitting Jobs.</description></item><item><title>Examining Partitions</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/examining-partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/examining-partitions/</guid><description>When logging into a new Slurm cluster you can examine characteristics of available partitions with the sinfo command. Below are some useful examples for Lane. Your available partition lists may look different.
# List names of partitions you have access to sinfo -o &amp;#34;%20P&amp;#34; # output: # # PARTITION # DEBUG # short1 # interactive # moh1 # moh2-gpu # pool1 # pool3-bigmem # List total resources for each partition # CPUS and MEMORY are given per node # NODES are given in the format Allocated/Idle sinfo -o &amp;#34;%20P %10A %10c %10m %25f %20G %20N&amp;#34; # output: # # PARTITION NODES(A/I) CPUS MEMORY AVAIL_FEATURES GRES NODELIST # DEBUG 0/0 0 0 (null) (null) # short1 0/3 8 32000+ rack-0,8CPUs (null) compute-0-[9-11] # short1 1/2 32 64153 rack-3,32CPUs (null) compute-3-[32-34] # interactive 0/3 8 32000+ rack-0,8CPUs (null) compute-0-[9-11] # moh1 2/3 48 257364 rack-0,48CPUs (null) compute-0-[7,20-21,2 # moh1 0/2 56 128530 rack-1,56CPUs (null) compute-1-[2-6] # moh2-gpu 0/1 32 257348 rack-4,32CPUs, gpu:4 compute-4-18 # pool1 0/4 16 48059 rack-0,16CPUs (null) compute-0-[4-6,8] # pool1 0/8 8 23936 rack-0,8CPUs (null) compute-0-[22-28,30] # pool1 0/11 8 23933+ rack-1,8CPUs (null) compute-1-[7-10,20-2 # pool1 0/5 16 48059 rack-1,16CPUs (null) compute-1-[14-18] # pool3-bigmem 0/1 32 257757 rack-0,32CPUs (null) compute-0-12 # pool3-bigmem 0/1 8 128761 rack-1,8CPUs (null) compute-1-35 # List free resources # CPUS in the format Allocated/Idle/Other/Total sinfo -o &amp;#34;%20P %20e %20m %20C %f&amp;#34; # output: # # PARTITION FREE_MEM MEMORY CPUS(A/I/O/T) AVAIL_FEATURES # DEBUG 0 0 0/0/0/0 (null) # short1 29863-45543 32000+ 0/24/0/24 rack-0,8CPUs # short1 39072-61998 64153 16/80/0/96 rack-3,32CPUs # interactive 29863-45543 32000+ 0/24/0/24 rack-0,8CPUs # moh1 29816-250476 257364 148/92/240/480 rack-0,48CPUs # moh1 34069-94091 128530 0/168/112/280 rack-1,56CPUs # moh2-gpu 167177 257348 1/31/0/32 rack-4,32CPUs, # pool1 13063-N/A 23933+ 2/86/8/96 rack-1,8CPUs # pool1 40995-45646 48059 0/64/0/64 rack-0,16CPUs # pool1 21716-23147 23936 0/64/0/64 rack-0,8CPUs # pool1 30473-45375 48059 3/77/0/80 rack-1,16CPUs # pool3-bigmem 37394 128761 1/7/0/8 rack-1,8CPUs # pool3-bigmem 254574 257757 0/32/0/32 rack-0,32CPUs Shared Partitions on Lane Partition Name Description short1 I&amp;rsquo;ve never used this.</description></item><item><title>Job Dependencies</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/job-dependencies/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/job-dependencies/</guid><description>You may end up wanting to have jobs wait for other jobs before they execute, either to guarantee some preprocessing or conserve cluster resources. This can be accomplished with the -d flag of sbatch. The -d flag takes values of the format type:job_id.
-d Type Explanation after After job ID has begun, not ended afterany After job ID has finished, regardless of error status afterok After job ID has finished and has not exited with an error afternotok After job ID has finished and has exited with an error aftercorr After array task of job ID corresponding with this array job&amp;rsquo;s array task has finished successfully.</description></item><item><title>Module Command Summary</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/summary/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/summary/</guid><description> Command Description module load MODULE Activates a module MODULE, running its modulefile module unload MODULE Removes an active module MODULE, undoing everything done in its modulefile module add MODULE Same as module load MODULE module rm MODULE Same as module unload MODULE module show MODULE Show what module MODULE will do when loaded module list Show all loaded modules module avail Show all available modules module purge Unload all loaded modules module use DIR Use DIR as a modulefile directory</description></item><item><title>Storage on Lane</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/storage/</guid><description>There are a few storage systems on Lane. First is your home directory (~, /home/$(whoami)). Home directories have 500GB are by default are only accessible by you. You can examine characteristics of your home directory with the command
df -ah | grep -P &amp;#34;(Filesystem|$(whoami))&amp;#34; # output: # # Filesystem Size Used Avail Use% Mounted on # nas-3-31:/volume01/home/mguler 500G 54G 447G 11% /home/mguler In addition to your home directory you will likely have access to network attached storage (NAS) filesystems specific to your lab.</description></item><item><title>Stopping Jobs</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/stopping-jobs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/stopping-jobs/</guid><description>If you want to stop your job first find your job ID with squeue then run scancel JOBID. For array jobs using just the job ID before the underscore you will cancel all array tasks in the array job. Using the underscore you will only cancel a single array step.
External Resources scancel man page</description></item><item><title/><link>https://mustafa-guler.github.io/lane_custer/dependency-management/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/</guid><description>Reading This Document Conda contains information about installing conda, creating conda environments, activating conda environments, and transferring conda environments across machines.
Conda conda is a python-oriented environment manager. While it is primarily geared towards python it can be used for many things. conda packages are pre-compiled for a particular system, so you will not need to deal with compiling your own dependencies. conda environments sandbox dependencies from each other so different environments can exist and be used without polluting the global package space.</description></item><item><title>Defining Terms</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/defining-terms/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/defining-terms/</guid><description>There are some terms I use throughout that should be explained first to remove ambiguity.
Term Definition cluster A remote server that consists of a collection of computers resources CPUs, memory, GPUs, etc. that can be consumed by a program node A single computer in a cluster, contains resources partition A collection of nodes in a cluster with shared characteristics job Some executable submitted to the cluster to run on some partition with some resources.</description></item><item><title>Installing Conda</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/installing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/conda/installing/</guid><description>You may already have conda installed under a shared FS for your lab. Check if this is the case before proceeding. If you use this installation please make sure you do not modify existing environments, as it could break other users&amp;rsquo; workflows.
To install your own copy go to the Miniconda installation page and copy the link to the latest linux installer. As of writing this it is for Python 3.</description></item><item><title>Lane's Default Modules</title><link>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/lane-defaults/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/dependency-management/modules/lane-defaults/</guid><description>Without any other modification you can run module avail to see what default modules are available on Lane.
module avail # output: # # ------------------------------------------------------- /usr/share/Modules/modulefiles -------------------------------------------------------- # cuda-10.0 matlab-9.5 R-3.5.3 singularity/ImageMagick-v6.9.10-23 # cuda-8.0 matlab-9.7 rocks-openmpi singularity/KNIME-v4.1.1 # cudnn-10.0-7.3 module-git rocks-openmpi_ib singularity/meme-suite-v5.1.0 # cudnn-8.0-7.1 module-info singularity/bedtools-v2.29.2 singularity/neofetch-v7.0.0 # dot modules singularity/bftools-v6.3.1 singularity/r-base-v3.6.2 # ffmpeg-4.2.2 null singularity/cellorganizer-v2.8.1 singularity/samtools-v1.10 # gurobi902 opt-perl singularity/cowsay-v3.03 singularity/texlive-v3.14 # java-1.8.0 python27 singularity/FastQC-v0.11.9 use.own # maple2019 python27-extras singularity/gcc-v8.</description></item><item><title>More Job Details</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/monitoring-jobs/more-job-details/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/monitoring-jobs/more-job-details/</guid><description>We have some options to get more details about jobs. Looking at my train_zinc job we can break down status by task with
sacct -j 349388 # output: # # JobID JobName Partition Account AllocCPUS State ExitCode # ------------ ---------- ---------- ---------- ---------- ---------- -------- # 349388 train_zin+ moh2-gpu local 1 RUNNING 0:0 # 349388.batch batch local 1 RUNNING 0:0 # 349388.exte+ extern local 1 RUNNING 0:0 # 349388.1 python local 1 RUNNING 0:0 and get all the job&amp;rsquo;s configuration with</description></item><item><title>sbatch vs srun</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/sbatch-vs-srun/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/submitting-jobs/sbatch-vs-srun/</guid><description>These two commands take almost the exact same parameters. The difference is sbatch submits jobs to the Slurm scheduler, to be run when requested resources become available. Running the same job directly with srun will actually run synchronously. In other words, you will have to sit and wait while the job runs and logging out of the cluster will likely terminate your job. It is generally not recommended to directly execute jobs with srun, one exception can be see in Interactive Sessions</description></item><item><title>scp</title><link>https://mustafa-guler.github.io/lane_custer/slurm-basics/transferring-data/scp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://mustafa-guler.github.io/lane_custer/slurm-basics/transferring-data/scp/</guid><description>The simplest way to transfer files is via scp. This functions similarly to cp, but operates over a network connection. It will respect your ssh configuration, so you can use Host definitions in the commands. For example, if you have set up the Lane ssh configuration entry as suggested you can do the following to download a file to your current local directory:
scp lane:/PATH/TO/FILE . Uploading operates similarly.
scp /PATH/TO/FILE lane:/PATH/TO/REMOTE/DESTINATION For directories use the -r flag to download/upload recursively.</description></item></channel></rss>